# LLM-reading-group
LLM reading group discussed at Houston Machine Learning Meetup

## Paper Reading
- Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning. [paper](https://pages.github.com/), [recording](https://youtu.be/3NBp_us1Lww)
- LoRA: Low-Rank Adaptation of Large Language Models. [paper](https://pages.github.com/), [recording](https://youtu.be/nFh0uRqlWjw)
- QLORA: Efficient Finetuning of Quantized LLMs. [paper](https://pages.github.com/), [recording](https://youtu.be/nFh0uRqlWjw)
- Automatic multi-step reasoning and tool-use for large language models. [paper](https://pages.github.com/), [recording](https://youtu.be/BFrrii1G9lQ)
- Winning solution from Nvidia for the Amazon 2024 KDD cup. [paper](https://pages.github.com/), [recording](https://youtu.be/0rVId3DGEeY)
- Inference Optimization for Foundation Models on AI Accelerators: Model Quantization and Distillation. [paper](https://pages.github.com/), [recording](https://youtu.be/9nmmfjM-b8g)
- Inference Optimization for Foundation Models on AI Accelerators: Architecture, KV cache and Flash attention. [paper](https://pages.github.com/), [recording](https://youtu.be/jk2FsJxZFo8)
- Mixtral of Experts. [paper](https://pages.github.com/), [recording](https://youtu.be/nw8ByURxaSQ)
- Llama family: Alpaca Vicuna and LLaVA. [paper](https://pages.github.com/), [recording](https://youtu.be/za4yminmZ-w)
- Llama 2: Open Foundation and Fine-Tuned Chat Models. [paper](https://pages.github.com/), [recording](https://youtu.be/cr42NH4cDEQ)
- LLaMA: Open and Efficient Foundation Language Models. [paper](https://pages.github.com/), [recording](https://youtu.be/A6h7shg8rEw)
- GPT-4. [paper](https://pages.github.com/), [recording](https://youtu.be/gqP39VvAW-c)
- InstructGPT: Training language models to follow instructions with human feedback. [paper](https://pages.github.com/), [recording](https://youtu.be/CnjCHNH0asw)
- GPT 3 - Language Models are Few-Shot Learners. [paper](https://pages.github.com/), [recording](https://youtu.be/w53U1ypIFIc)
- Scaling Laws for Neural Language Models. [paper](https://pages.github.com/), [recording](https://youtu.be/TZtk1B7K0Qk)
- T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. [paper](https://pages.github.com/), [recording](https://youtu.be/_uAMvBBYMu8)
- Secrets in Training a Large Language Model: Pretraining, Fine-tuning and RLHF. [recording](https://youtu.be/cybEKSNBp-w)
- Introduction to Transformers. [paper](https://pages.github.com/), [recording](https://youtu.be/k6LrtDzrpWw)

## Hands-on Session
- Hands-on session with Fine-tuning a LLM. [notebook](https://pages.github.com/), [recording](https://youtu.be/ZRQBCSZR-5E)
- Hands-on session of RAG with Llama 3. [notebook](https://pages.github.com/), [recording](https://youtu.be/ooz8juF2e3I)
