# LLM reading group
LLM reading group discussed at Houston Machine Learning Meetup

## Paper Reading
- Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning. [paper](https://arxiv.org/abs/2205.05638), [recording](https://youtu.be/3NBp_us1Lww)
- LoRA: Low-Rank Adaptation of Large Language Models. [paper](https://arxiv.org/abs/2106.09685), [recording](https://youtu.be/nFh0uRqlWjw)
- QLORA: Efficient Finetuning of Quantized LLMs. [paper](https://arxiv.org/abs/2305.14314), [recording](https://youtu.be/nFh0uRqlWjw)
- Automatic multi-step reasoning and tool-use for large language models. [paper](https://arxiv.org/abs/2303.09014), [recording](https://youtu.be/BFrrii1G9lQ)
- Amazon 2024 KDD cup Winning solution from Nvidia. [paper](https://openreview.net/pdf?id=sv0E1mBhu8), [recording](https://youtu.be/0rVId3DGEeY)
- Inference Optimization for Foundation Models on AI Accelerators: Model Quantization and Distillation. [paper](https://drive.google.com/file/d/1uVhHtRBwXy7o8ejaS6Ab6pSybkzticE3/view), [recording](https://youtu.be/9nmmfjM-b8g)
- Inference Optimization for Foundation Models on AI Accelerators: Architecture, KV cache and Flash attention. [paper](https://drive.google.com/file/d/1uVhHtRBwXy7o8ejaS6Ab6pSybkzticE3/view), [recording](https://youtu.be/jk2FsJxZFo8)
- Mixtral of Experts. [paper](https://arxiv.org/abs/2401.04088), [recording](https://youtu.be/nw8ByURxaSQ)
- Textbooks are All You Need: Phi 1 & 1.5. [paper](https://arxiv.org/abs/2306.11644), [recording](https://youtu.be/VLoLFHg7aQM)
- Llama family: Alpaca Vicuna and LLaVA. [paper](https://arxiv.org/abs/2304.08485), [recording](https://youtu.be/za4yminmZ-w)
- Llama 2: Open Foundation and Fine-Tuned Chat Models. [paper](https://arxiv.org/abs/2307.09288), [recording](https://youtu.be/cr42NH4cDEQ)
- LLaMA: Open and Efficient Foundation Language Models. [paper](https://arxiv.org/abs/2302.13971), [recording](https://youtu.be/A6h7shg8rEw)
- GPT-4. [paper](https://arxiv.org/abs/2303.08774), [recording](https://youtu.be/gqP39VvAW-c)
- InstructGPT: Training language models to follow instructions with human feedback. [paper](https://arxiv.org/abs/2203.02155), [recording](https://youtu.be/CnjCHNH0asw)
- GPT 3 - Language Models are Few-Shot Learners. [paper](https://arxiv.org/abs/2005.14165), [recording](https://youtu.be/w53U1ypIFIc)
- Scaling Laws for Neural Language Models. [paper](https://arxiv.org/abs/2001.08361), [recording](https://youtu.be/TZtk1B7K0Qk)
- T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. [paper](https://arxiv.org/abs/1910.10683), [recording](https://youtu.be/_uAMvBBYMu8)
- Secrets in Training a Large Language Model: Pretraining, Fine-tuning and RLHF. [recording](https://youtu.be/cybEKSNBp-w)
- Introduction to Transformers. [paper](https://arxiv.org/abs/1706.03762), [recording](https://youtu.be/k6LrtDzrpWw)

## Hands-on Session
- Hands-on session with Fine-tuning a LLM. [notebook](https://colab.research.google.com/drive/1QNOyWzD8H7xA6_ulVeL-5NIXWqMMR7kp?usp=drive_link), [recording](https://youtu.be/ZRQBCSZR-5E)
- Hands-on session of RAG with Llama 3. [notebook](https://colab.research.google.com/drive/1CuohoBl31hcAKuRdTYwxGY374v0Mc7uV?usp=drive_link), [recording](https://youtu.be/ooz8juF2e3I)

## Backlog
