# LLM-reading-group
LLM reading group discussed at Houston Machine Learning Meetup

## Paper Reading
- Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- LoRA: Low-Rank Adaptation of Large Language Models. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- QLORA: Efficient Finetuning of Quantized LLMs. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- Automatic multi-step reasoning and tool-use for large language models. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- Winning solution from Nvidia for the Amazon 2024 KDD cup. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- Inference Optimization for Foundation Models on AI Accelerators. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- Mixtral of Experts. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- Llama family: Alpaca Vicuna and LLaVA. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- Llama 2: Open Foundation and Fine-Tuned Chat Models. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- LLaMA: Open and Efficient Foundation Language Models. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- GPT-4. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- InstructGPT: Training language models to follow instructions with human feedback. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- GPT 3 - Language Models are Few-Shot Learners. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- Scaling Laws for Neural Language Models. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- Secrets in Training a Large Language Model. [slides](https://pages.github.com/), [recording](https://pages.github.com/)
- Introduction to Transformers. [slides](https://pages.github.com/), [recording](https://pages.github.com/)

## Hands-on Session
- Hands-on session with Fine-tuning a LLM. [notebook](https://pages.github.com/), [recording](https://pages.github.com/)
- Hands-on session of RAG with Llama 3. [notebook](https://pages.github.com/), [recording](https://pages.github.com/)
