# LLM reading group
LLM reading group discussed at [Houston Machine Learning Meetup](https://www.meetup.com/houston-machine-learning/?eventOrigin=event_home_page)

## Paper Reading
- DeepSeek R1: Incentivizing Reasoning Capability in LLMs. [paper](https://arxiv.org/abs/2501.12948), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/a91d35520f98d444cd3f52601585ecc5a6eb143b/presentation-slides/DeepSeek-R1.pdf), [recording](https://youtu.be/AEN_D3UIFvY?si=pQUQpzgYY6CiqmTx)
- DeepSeek V3: Architecture and Design. [paper](https://arxiv.org/pdf/2412.19437), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/c4970f07c99c031d0164a86054d2a120bd943a45/presentation-slides/DeepSeek-V3.pdf), [recording](https://youtu.be/pGn7h86V230?si=o-hL7nYOKtKXVFsH)
- Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning. [paper](https://arxiv.org/abs/2205.05638), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/Few-shot-fine-tuning.pdf), [recording](https://youtu.be/3NBp_us1Lww)
- LoRA: Low-Rank Adaptation of Large Language Models. [paper](https://arxiv.org/abs/2106.09685), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/LoRA_and_QLoRA.pdf), [recording](https://youtu.be/nFh0uRqlWjw)
- QLORA: Efficient Finetuning of Quantized LLMs. [paper](https://arxiv.org/abs/2305.14314), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/LoRA_and_QLoRA.pdf), [recording](https://youtu.be/nFh0uRqlWjw)
- Automatic multi-step reasoning and tool-use for large language models. [paper](https://arxiv.org/abs/2303.09014), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/Automatic%20Multi-step%20Reasoning%20and%20Tool-use.pdf), [recording](https://youtu.be/BFrrii1G9lQ)
- Amazon 2024 KDD cup Winning solution from Nvidia. [paper](https://openreview.net/pdf?id=sv0E1mBhu8), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/NVIDIA-KDD2024-Slides.pdf), [recording](https://youtu.be/0rVId3DGEeY)
- Inference Optimization for Foundation Models on AI Accelerators: Model Quantization and Distillation. [paper](https://drive.google.com/file/d/1uVhHtRBwXy7o8ejaS6Ab6pSybkzticE3/view), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/LLM_inference_optimization_model_compression.pdf), [recording](https://youtu.be/9nmmfjM-b8g)
- Inference Optimization for Foundation Models on AI Accelerators: Architecture, KV cache and Flash attention. [paper](https://drive.google.com/file/d/1uVhHtRBwXy7o8ejaS6Ab6pSybkzticE3/view), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/LLM_inference_optimization_architecture_KVcache_flash_attention.pdf), [recording](https://youtu.be/jk2FsJxZFo8)
- Mistral and Mixtral of Experts. [paper](https://arxiv.org/abs/2401.04088), [slides](), [recording](https://youtu.be/nw8ByURxaSQ)
- Textbooks are All You Need: Phi 1 & 2. [paper](https://arxiv.org/abs/2306.11644), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/Phi%20model%20series.pdf), [recording](https://youtu.be/VLoLFHg7aQM)
- Llama family: Alpaca Vicuna and LLaVA. [paper](https://arxiv.org/abs/2304.08485), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/LlamaFamliy.pdf), [recording](https://youtu.be/za4yminmZ-w)
- Llama 2: Open Foundation and Fine-Tuned Chat Models. [paper](https://arxiv.org/abs/2307.09288), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/llama2.pdf), [recording](https://youtu.be/cr42NH4cDEQ)
- LLaMA: Open and Efficient Foundation Language Models. [paper](https://arxiv.org/abs/2302.13971), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/llama1.pdf), [recording](https://youtu.be/A6h7shg8rEw)
- GPT-4. [paper](https://arxiv.org/abs/2303.08774), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/GPT4.pdf), [recording](https://youtu.be/gqP39VvAW-c)
- InstructGPT: Training language models to follow instructions with human feedback. [paper](https://arxiv.org/abs/2203.02155), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/InstructGPT.pdf), [recording](https://youtu.be/CnjCHNH0asw)
- GPT 3 - Language Models are Few-Shot Learners. [paper](https://arxiv.org/abs/2005.14165), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/GPT3.pdf), [recording](https://youtu.be/w53U1ypIFIc)
- Scaling Laws for Neural Language Models. [paper](https://arxiv.org/abs/2001.08361), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/scaling%20Laws.pdf), [recording](https://youtu.be/TZtk1B7K0Qk)
- T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. [paper](https://arxiv.org/abs/1910.10683), [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/T5.pdf), [recording](https://youtu.be/_uAMvBBYMu8)
- Secrets in Training a Large Language Model: Pretraining, Fine-tuning and RLHF. [slides](https://github.com/YanXuHappygela/LLM-reading-group/blob/e41c398a6dd38a1692271f12b2844bf4ea581d86/presentation-slides/secrets_in_training_LLM.pdf) [recording](https://youtu.be/cybEKSNBp-w)
- Introduction to Transformers. [paper](https://arxiv.org/abs/1706.03762), [recording](https://youtu.be/k6LrtDzrpWw)

## Hands-on Session
- Hands-on session with Fine-tuning a LLM. [notebook](https://colab.research.google.com/drive/1QNOyWzD8H7xA6_ulVeL-5NIXWqMMR7kp?usp=drive_link), [recording](https://youtu.be/ZRQBCSZR-5E)
- Hands-on session of RAG with Llama 3. [notebook](https://colab.research.google.com/drive/1CuohoBl31hcAKuRdTYwxGY374v0Mc7uV?usp=drive_link), [recording](https://youtu.be/ooz8juF2e3I)

## Backlog
